# Algorithms Explained:
* An algorithm is usually made up of 3 building blocks (sequencing, selection, and iteration). Sequencing is a step-by-step process in which the order of the steps are crucial to ensure the correctness of the algorithm. Selection is when algorithms pick which steps to execute based on Boolean expression(s). Iteration is iterating a step until a certain requirment is met.

<strong>Verifying An Algorithm:</strong>
1. <strong>Empiricial Analysis</strong> - Empirical Analysis is a verification method based on actual experimentation and observation of the results. This means the algorithm must be translated to a programming language and executed. Empirical analysis can only be used to prove an algorithm is *not correct* when discovering inputs where the output is unexpected. But, it cannot prove an algorithm is correct.
2. <strong>Formal Reasoning</strong> - The only way to prove the correctness of an algorithm over all possible inputs is via formal/mathematical reasoning. *Induction* is one type of mathemical reasoning in which the proof goes along the lines of if the output is correct for the 1st input then it must be correct for the rest of the inputs (you can write out the proof). Most programmers lack the theoretical skills for these types of proof so they use empirical analysis.

<strong>Measuring Algorithm Efficiency:</strong>
* A good algorithm is correct but its, also, efficient, taking the least amount of execution time and memory usage as possible.
* *Counting Operations* - You can count how many operations/steps it takes your algorithm to figure out the correct output(s), and you can do this for your best (least amount of steps), worst (most amount of steps), and average scenarios and plot this on a graph. The line can reveal whether its a linear, exponential, and more relationship and whether as the number of inputs increases does the steps increase.
* *Running time* - Run time depends on the speed of the computer, programming language, and translating the language to machine code. But, it can still be helpful to figure out an algorithm's efficiency. You can record the time it takes for the algorithm to execute and can plot it, observing the relationship. ALso, time complexity can be used to mathematically figure out the run time.
* There is a big difference in run times, for some algorithms, based on how the input sizes increase. *Constant size* means the algorithm takes a constant time for the alorithm to execute, no matter the input size, the graph is a straight line. *Logarithmic time* is whent the time increases proportionally to the logarithm of the input size (like a binary search algorithm). *Quadratic time* is when an algorithm's steps increase in proportion to the input size squared. *Exponential time* is when an algorithm grows in superpolynomial time.
* Run times are usually classified into 2 classes (polynomial and superpolynomial). *Polynomial* time is any run time that doesn't increase faster than n<sup>k</sup> (including constant time, n<sup>0</sup>, logarithmic time, log<sub>2</sub>n, and more). *Superpolynomial* time is any run time that increases faster than n<sup>k</sup> (including exponential time, 2<sup>n</sup>, factorial time, n!, and more). Superpolynomial run times usually require more time than avaliable in the universe, even for relativley small input sizes. That's why polynomial run times are reasonable and superpolynomial times are unreasonable.

<strong>Heuristic:</strong>
* Since superpolynomial algorithms take an unbelievable amount of time for relativley small input sizes computer scientists try to come up with an approximate solution. One way is to use a *heuristic* technique which is a technique that guides an algorithm to find good choices. The algorithm doesn't need to exhaustively search every possible solution and can find more approximate solutions more quickly.
* An example of a problem solved with a heuristic approach is the Traveling Salesman Problem (TSP), the problem goes along the line you are a traveling salesman and you need to find the shortest path between 46 cities, placed in any type of assortment. This represents a superpolynomial run time as the number of cities increase, the run time for the algorithm increases exponentially. The heursitic solution would be to start off on 1 city then find the closest city to that city and continue that for the next city and so on until you come back to the original city. Even though this isn't the perfect solution some inputs, this a close approximation.

<strong>Parallel Computing:</strong>
* The standard programming model (the sequential model) executes one step at a time but modern computers with multi-core processors can have each core, independently execute an operation/step. This is called *parallel computing*, a computational model that breaks programs into smaller sequential operations and performs those smaller operations in parallel.
* One way to evaluate the benefits of parallelizing a program is to measure the *speedup*, the ratio of the time taken to run the program sequentially to the time taken to run the parallelized program.
* *Hyperthreading* is a technology (by Intel) that allow a single CPU core to run 2 threads concurrently, this works well when the 2 threads are doing different types of computations.

<strong>Distributed Computing:</strong>
* *Distributed computing* is to distribute problems across multiple networked computing devices, its often used in tandem with parallel computing. Distributed computing can improve the performance of many solutions by taking advantage of hundres or thousands of networked computers running in parallel.
* The speedup can evaluate how much better distributed computing performs.
* Distrbuted computing can also allow for distributed functionality in which some computer devices perform some certain functions while other computer devices perform other functions.
